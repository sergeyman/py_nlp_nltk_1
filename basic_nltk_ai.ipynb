{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFKw5pexoWVqQfw6kWCYGe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sergeyman/py_nlp_nltk_1/blob/main/basic_nltk_ai.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4SQ0H4emUkn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "754c6071-b400-4e31-a0e5-d5ac7d7c1b99"
      },
      "source": [
        "import nltk\n",
        "! pip3 install pattern"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pattern\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/07/b0e61b6c818ed4b6145fe01d1c341223aa6cfbc3928538ad1f2b890924a3/Pattern-3.6.0.tar.gz (22.2MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Collecting backports.csv\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/26/a6bd68f13e0f38fbb643d6e497fc3462be83a0b6c4d43425c78bb51a7291/backports.csv-1.0.7-py2.py3-none-any.whl\n",
            "Collecting mysqlclient\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/df/59cd2fa5e48d0804d213bdcb1acb4d08c403b61c7ff7ed4dd4a6a2deb3f7/mysqlclient-2.0.3.tar.gz (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 8.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Collecting feedparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/21/faf1bac028662cc8adb2b5ef7a6f3999a765baa2835331df365289b0ca56/feedparser-6.0.2-py3-none-any.whl (80kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.9MB/s \n",
            "\u001b[?25hCollecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 21.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Collecting python-docx\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/83/c66a1934ed5ed8ab1dbb9931f1779079f8bca0f6bbc5793c06c4b5e7d671/python-docx-0.8.10.tar.gz (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 45.8MB/s \n",
            "\u001b[?25hCollecting cherrypy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/f9/e11f893dcabe6bc222a1442bf5e14f0322a2d363c92910ed41947078a35a/CherryPy-18.6.0-py2.py3-none-any.whl (419kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 38.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Collecting sgmllib3k\n",
            "  Downloading https://files.pythonhosted.org/packages/9e/bd/3704a8c3e0942d711c1299ebf7b9091930adae6675d7c8f476a7ce48653c/sgmllib3k-1.0.0.tar.gz\n",
            "Collecting cryptography\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (2.3.0)\n",
            "Requirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->pattern) (1.15.0)\n",
            "Collecting zc.lockfile\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/2a/268389776288f0f26c7272c70c36c96dcc0bdb88ab6216ea18e19df1fadd/zc.lockfile-2.0-py2.py3-none-any.whl\n",
            "Collecting cheroot>=8.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/95/86fe6480af78fea7b0e7e1bf02e6acd4cb9e561ea200bd6d6e1398fe5426/cheroot-8.5.2-py2.py3-none-any.whl (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.2MB/s \n",
            "\u001b[?25hCollecting portend>=2.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/fd29409cced540facdd29abb986d988cb1f22c8170d10022ea73af77fa55/portend-2.7.1-py3-none-any.whl\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.7.0)\n",
            "Collecting jaraco.collections\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1a/a0d6861d2aca6df92643c755966c8a60e40353e4c5e7a5c2f4e5ed733817/jaraco.collections-3.3.0-py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2020.12.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.14.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (54.2.0)\n",
            "Collecting jaraco.functools\n",
            "  Downloading https://files.pythonhosted.org/packages/b5/da/e51e7b58c8fe132990edd1e3ef25bcd9801eb7f91d0f642ac7f8d97e4a36/jaraco.functools-3.3.0-py3-none-any.whl\n",
            "Collecting tempora>=1.8\n",
            "  Downloading https://files.pythonhosted.org/packages/06/e0/b2a0c95bebd29c757b332a2a373e8cc0debcaba801ae5dc5b7d03db1979f/tempora-4.0.1-py3-none-any.whl\n",
            "Collecting jaraco.text\n",
            "  Downloading https://files.pythonhosted.org/packages/c1/74/2a3c4835c079df16db8a9c50263eebb0125849fee5b16de353a059b7545d/jaraco.text-3.5.0-py3-none-any.whl\n",
            "Collecting jaraco.classes\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/74/bee5fc11594974746535117546404678fc7b899476e769c3c55bc0cfaa02/jaraco.classes-3.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.20)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2018.9)\n",
            "Building wheels for collected packages: pattern, mysqlclient, python-docx, sgmllib3k\n",
            "  Building wheel for pattern (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pattern: filename=Pattern-3.6-cp37-none-any.whl size=22332724 sha256=a3505be2cd6b13ca653e9c6e3ef4921dd5692e2cc8b02af2c8b570910e29bda0\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/9a/0e/5fb1a603ed4e3aa8722a88e9cf4a82da7d1b63e3d2cc34bee5\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.0.3-cp37-cp37m-linux_x86_64.whl size=100095 sha256=0d2ff38c29ddb958b9fe135fc573d42962d2fac8deff949e30b900cfdcb33e8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/ca/e8/ad4e7ce3df18bcd91c7d84dd28c7c08db491a2a2360efed363\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.10-cp37-none-any.whl size=184491 sha256=f5152ea18f602dd5b831217e58163c3a125411904f97f93064f5bd9d6212f7e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/0b/a0/1dd62ff812c857c9e487f27d80d53d2b40531bec1acecfa47b\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-cp37-none-any.whl size=6067 sha256=573fe231c8a821df2efbd730f5c31ef228ce711aa9c956644f42a536a37080f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/80/5a/444ba08a550cdd241bd9baf8bae44be750efe370adb944506a\n",
            "Successfully built pattern mysqlclient python-docx sgmllib3k\n",
            "Installing collected packages: backports.csv, mysqlclient, sgmllib3k, feedparser, cryptography, pdfminer.six, python-docx, zc.lockfile, jaraco.functools, cheroot, tempora, portend, jaraco.text, jaraco.classes, jaraco.collections, cherrypy, pattern\n",
            "Successfully installed backports.csv-1.0.7 cheroot-8.5.2 cherrypy-18.6.0 cryptography-3.4.7 feedparser-6.0.2 jaraco.classes-3.2.1 jaraco.collections-3.3.0 jaraco.functools-3.3.0 jaraco.text-3.5.0 mysqlclient-2.0.3 pattern-3.6 pdfminer.six-20201018 portend-2.7.1 python-docx-0.8.10 sgmllib3k-1.0.0 tempora-4.0.1 zc.lockfile-2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsJe_PJTmdU6"
      },
      "source": [
        "import gensim\n",
        "import pattern\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_ekKbQenydL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a325f75e-1f53-46c3-e423-7921ed0f45a1"
      },
      "source": [
        "# 1) Токенизация текстовых данных\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, WordPunctTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, \\\n",
        "        word_tokenize, WordPunctTokenizer\n",
        "\n",
        "## RU: stop-words\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "# stopwords.words(\"russian\")\n",
        "# len(stopwords.words(\"russian\"))\n",
        "stop_words = stopwords.words(\"russian\")\n",
        "\n",
        "\n",
        "# Define input text\n",
        "# input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\" \n",
        "## RU: \n",
        "input_text = \"Я — к.т.н, т.е. проучился долгое время. Имею образование.\" \n",
        "\n",
        "# Sentence tokenizer \n",
        "print(\"\\nSentence tokenizer:\")\n",
        "# print(sent_tokenize(input_text))\n",
        "## RU: \n",
        "print(sent_tokenize(input_text, language=\"russian\"))\n",
        "# Как видим, функция sent_tokenize разбила исходное предложения на два, несмотря на присутствие слов к.т.н. и т.е.\n",
        "\n",
        "# Word tokenizer\n",
        "print(\"\\nWord tokenizer:\")\n",
        "# print(word_tokenize(input_text))\n",
        "## RU: \n",
        "input_text = \"Я — к.т.н. Сижу на диван-кровати.\"\n",
        "print(word_tokenize(input_text, language=\"russian\"))\n",
        "# Здесь к.т.н. и диван-кровать были определены как отдельные слова.\n",
        "\n",
        "# WordPunct tokenizer\n",
        "print(\"\\nWord punct tokenizer:\")\n",
        "print(WordPunctTokenizer().tokenize(input_text))\n",
        "\n",
        "## RU: stopwords\n",
        "#tokens = sent_tokenize(input_text)\n",
        "tokens = word_tokenize(input_text)\n",
        "filtered_tokens = []\n",
        "for token in tokens:\n",
        "  if token not in stop_words:\n",
        "    filtered_tokens.append(token)\n",
        "\n",
        "print(\"\\nGet rid of stopwords: \")\n",
        "print(filtered_tokens)\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "\n",
            "Sentence tokenizer:\n",
            "['Я — к.т.н, т.е. проучился долгое время.', 'Имею образование.']\n",
            "\n",
            "Word tokenizer:\n",
            "['Я', '—', 'к.т.н.', 'Сижу', 'на', 'диван-кровати', '.']\n",
            "\n",
            "Word punct tokenizer:\n",
            "['Я', '—', 'к', '.', 'т', '.', 'н', '.', 'Сижу', 'на', 'диван', '-', 'кровати', '.']\n",
            "\n",
            "Get rid of stopwords: \n",
            "['Я', '—', 'к.т.н', '.', 'Сижу', 'диван-кровати', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwKOa2cYn_qS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdb887af-c245-42d4-f0b9-128631029d0f"
      },
      "source": [
        "# 2) Преобразование слов в их базовые формы с помощью стемминга\n",
        "# (приведение слов в их различных формах к общему корню.)\n",
        "from nltk.stem.porter import PorterStemmer            # наименее строгий\n",
        "from nltk.stem.lancaster import LancasterStemmer      # наиболее строгий\n",
        "from nltk.stem.snowball import SnowballStemmer        ## RU\n",
        "               \n",
        "# input_words = ['writing', 'calves', 'bе', 'branded', 'horse',\n",
        "# 'randomize', 'possiЬly', 'provision', 'hospital',\n",
        "# 'kept', 'scratchy', 'code', 'mice', 'feet', 'trying']\n",
        "\n",
        "# Русский и укр. не работает!!!\n",
        "input_words = ['работой', 'порядком', 'бачити', 'кошками', 'передние', \n",
        "               'смысловые', 'семерки', 'картинная', 'исключительный', 'возможности', \n",
        "               '', ]\n",
        "\n",
        "# Создание объектов различных стеммеров\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "# snowball = SnowballStemmer('english')\n",
        "snowball = SnowballStemmer(language=\"russian\")\n",
        "\n",
        "# Создание списка имен стеммеров для отображения\n",
        "stemmer_names = ['PORTER', 'LANCASTER', 'SNOWBALL']\n",
        "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
        "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), '\\n', '='*68)\n",
        "\n",
        "# Стемминг слов и отображение результатов\n",
        "for word in input_words:\n",
        "  output = [word, porter.stem(word), lancaster.stem(word), snowball.stem(word)]\n",
        "  print(formatted_text.format(*output))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "       INPUT WORD          PORTER       LANCASTER        SNOWBALL \n",
            " ====================================================================\n",
            "         работой         работой         работой           работ\n",
            "        порядком        порядком        порядком          порядк\n",
            "          бачити          бачити          бачити           бачит\n",
            "         кошками         кошками         кошками            кошк\n",
            "        передние        передние        передние          передн\n",
            "       смысловые       смысловые       смысловые         смыслов\n",
            "         семерки         семерки         семерки          семерк\n",
            "       картинная       картинная       картинная          картин\n",
            "  исключительный  исключительный  исключительный    исключительн\n",
            "     возможности     возможности     возможности         возможн\n",
            "                                                                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FuX0pYGwE3A"
      },
      "source": [
        "!pip install pymorphy2                ## RU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EepMKkv-2wAq",
        "outputId": "a39b7774-9e5f-42a5-eede-3d38b0bf1124"
      },
      "source": [
        "# 3) Преобразование слов в их корневые формы с помощью лемматизации\n",
        "# (более структурированный подход - удаление окончаний -ing, -ed -> баз. форма слова = лемма <calves -> calf>\n",
        "\n",
        "nltk.download('wordnet')              # additional on Error\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import pymorphy2                      ## RU\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "# Morph.parse(“хочу”)\n",
        "\n",
        "# input_words = ['writing', 'calves', 'bе', 'branded', 'horse',\n",
        "# 'randomize', 'possiЬly', 'provision', 'hospital',\n",
        "# 'kept', 'scratchy', 'code', 'mice', 'feet', 'trying']\n",
        "\n",
        "## RU\n",
        "input_words = ['работой', 'сподобалася ', 'бачити', 'визначено', 'передние', \n",
        "               'смысловые', 'результату', 'картинная', 'исключительный', 'возможности', \n",
        "               'важливий', ]\n",
        "\n",
        "# Создание объекта лемматизатора\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Создание списка имен лемматизаторов для их отображения\n",
        "lemmatizer_names = ['NOUN LEMМATIZER', 'VERB LEММATIZER']\n",
        "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
        "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), '\\n', '='*75)\n",
        "\n",
        "# Лемматизация слов и отображение результатов\n",
        "# for word in input_words:\n",
        "#   output = [word, lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='v')]\n",
        "#   print(formatted_text.format(*output))\n",
        "\n",
        "## RU\n",
        "for word in input_words:\n",
        "  output = [word, morph.parse(word)]\n",
        "#  print(formatted_text.format(*output))\n",
        "#  print(morph.parse(word))\n",
        "  print(word + ' - ' + morph.parse(word)[0].normal_form)\n",
        "\n",
        "## Метод parse возвращает список объектов Parse, которые обозначают виды грамматических форм анализируемого слова. \n",
        "# Такой объект обладает следующими атрибутами:\n",
        "# tag обозначает набор граммем. В данном случае слово \"хочу\" — это \n",
        "#   глагол (VERB) \n",
        "#   несовершенного вида (impf), \n",
        "#   переходный (tran), \n",
        "#   единственного числа (sing), \n",
        "#   1 лица (1per), \n",
        "#   настоящего времени (pres), \n",
        "#   изъявительного наклонения (indc);\n",
        "# normal_form– нормального форма слова;\n",
        "# score — оценка вероятности того, что данный разбор правильный;\n",
        "# methods_stack — тип словаря распарсенного слова с его индексом.\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\n",
            "               INPUT WORD         NOUN LEMМATIZER         VERB LEММATIZER \n",
            " ===========================================================================\n",
            "работой - работа\n",
            "сподобалася  - сподобалася \n",
            "бачити - бачить\n",
            "визначено - визначить\n",
            "передние - передний\n",
            "смысловые - смысловой\n",
            "результату - результат\n",
            "картинная - картинный\n",
            "исключительный - исключительный\n",
            "возможности - возможность\n",
            "важливий - важливия\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4eisgzyEtrD"
      },
      "source": [
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17_a6wQT0Bx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad03a763-cf3d-4b00-8eb1-a198c5731b41"
      },
      "source": [
        "# 3) Разбиение текстовых данных на информационные блоки (chunks)\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import brown    # Коллекция текстов\n",
        "\n",
        "# Разбиение входного текста на блоки, причем каждый блок содержит N слов\n",
        "def chunkier(input_data, N):\n",
        "  input_words = input_data.split(' ')\n",
        "  output = []\n",
        "\n",
        "  cur_chunk = []\n",
        "  count = 0\n",
        "  for word in input_words:\n",
        "    cur_chunk.append(word)\n",
        "    count += 1\n",
        "    if count == N:\n",
        "      output.append(' '.join(cur_chunk))\n",
        "      count, cur_chunk = 0, []\n",
        "  output.append(' '.join(cur_chunk))\n",
        "  return output\n",
        "\n",
        "\n",
        "# https://stackoverflow.com/questions/22923002/if-name-main-in-ipython\n",
        "# if __name__=='__main__':\n",
        "# if __name__ == '__main__' and '__file__' in globals():\n",
        "#     # Read the first 12000 words from the Brown corpus\n",
        "#     input_data = ' '.join(brown.words()[:12000])\n",
        "\n",
        "#     # Define the number of words in each chunk \n",
        "#     chunk_size = 700\n",
        "\n",
        "#     chunks = chunker(input_data, chunk_size)\n",
        "#     print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
        "#     for i, chunk in enumerate(chunks):\n",
        "#         print('Chunk', i+1, '==>', chunk[:50])\n",
        "\n",
        "# https://colab.research.google.com/github/rajat05jain/ExploreDataWithColab/blob/master/list1.ipynb#scrollTo=Hg8WXy0l978x\n",
        "def main():\n",
        "    # Read the first 12000 words from the Brown corpus\n",
        "    input_data = ' '.join(brown.words()[:12000])\n",
        "\n",
        "    # Define the number of words in each chunk \n",
        "    chunk_size = 700\n",
        "\n",
        "    chunks = chunkier(input_data, chunk_size)\n",
        "    print('\\nNumber of text chunks =', len(chunks), '\\n')\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print('Chunk', i+1, '==>', chunk[:150])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Number of text chunks = 18 \n",
            "\n",
            "Chunk 1 ==> The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took\n",
            "Chunk 2 ==> '' . ( 2 ) Fulton legislators `` work with city officials to pass enabling legislation that will permit the establishment of a fair and equitable '' p\n",
            "Chunk 3 ==> . Construction bonds Meanwhile , it was learned the State Highway Department is very near being ready to issue the first $30 million worth of highway \n",
            "Chunk 4 ==> , anonymous midnight phone calls and veiled threats of violence . The former county school superintendent , George P. Callan , shot himself to death M\n",
            "Chunk 5 ==> Harris , Bexar , Tarrant and El Paso would be $451,500 , which would be a savings of $157,460 yearly after the first year's capital outlay of $88,000 \n",
            "Chunk 6 ==> set it for public hearing on Feb. 22 . The proposal would have to receive final legislative approval , by two-thirds majorities , before March 1 to be\n",
            "Chunk 7 ==> College . He has served as a border patrolman and was in the Signal Corps of the U.S. Army . Denton , Texas ( sp. ) -- Principals of the 13 schools in\n",
            "Chunk 8 ==> of his staff were doing on the address involved composition and wording , rather than last minute decisions on administration plans to meet the latest\n",
            "Chunk 9 ==> plan alone would boost the base to $5,000 a year and the payroll tax to 6.5 per cent -- 3.25 per cent each . Similar payroll tax boosts would be impos\n",
            "Chunk 10 ==> nursing homes In the area of `` community health services '' , the President called for doubling the present 10 million dollar a year federal grants f\n",
            "Chunk 11 ==> of its Angola policy prove harsh , there has been a noticeable relaxation of tension . The general , remarkably courteous , explanation has left basic\n",
            "Chunk 12 ==> system which will prevent Laos from being used as a base for Communist attacks on neighboring Thailand and South Viet Nam . They count on the aid of t\n",
            "Chunk 13 ==> reform in recipient nations . In Laos , the administration looked at the Eisenhower administration efforts to show determination by sailing a naval fl\n",
            "Chunk 14 ==> . He is not interested in being named a full-time director . Noting that President Kennedy has handed the Defense Department the major responsibility \n",
            "Chunk 15 ==> said , `` to obtain the views of the general public and religious , labor and special-interest groups affected by these laws '' . The governor wrote M\n",
            "Chunk 16 ==> '' . Mr. Reama , far from really being retired , is engaged in industrial relations counseling . A petition bearing the signatures of more than 1,700 \n",
            "Chunk 17 ==> making enforcement of minor offenses more effective . Nothing has been done yet to take advantage of the enabling legislation . At present all offense\n",
            "Chunk 18 ==> to tell the people where he stands on the tax issue '' . Defends Ike Earlier , Mitchell said in a statement : `` I think that all Americans will resen\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}